import os, sys
import signal
import json
import traceback
import time
from collections import defaultdict
import pandas as pd
import requests
import threading
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from enum import Enum
import codecs
import datetime
from uniqa.logging import logDog as logger

# å¯¼å…¥ kafka-python çš„ç›¸å…³æ¨¡å—
from kafka import KafkaConsumer as KPConsumer  # ä½¿ç”¨åˆ«åä»¥é¿å…ä¸æˆ‘ä»¬çš„ç±»åå†²çª
from kafka.structs import TopicPartition
from kafka.errors import KafkaError

sys.path.append(r"../../")
from utils import RedisUtilsSentinel
from configs.config import paths
from configs.config import kafka_config, redis_config, snapshot_key

"""
# pip install kafka-python python-snappy
# https://github.com/dpkp/kafka-python?tab=readme-ov-file
# https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html
"""


# --- ä½¿ç”¨æšä¸¾ä»£æ›¿é­”æ³•å­—ç¬¦ä¸² ---
class SourceType(str, Enum):
    ENTITY = 'ENTITY'
    PARAM = 'PARAM'
    INTENT = 'INTENT'
    BOT_DEPLOY = 'BOT_DEPLOY'

class OperateType(str, Enum):
    INSERT = 'INSERT'
    UPDATE = 'UPDATE'
    DELETE = 'DELETE'
    # DEPLOY = 'DEPLOY'

# å¸¸é‡å®šä¹‰
SNAPSHOT_INTERVAL_SECONDS = 3600  # å¿«ç…§é—´éš”ï¼Œ3600ç§’ = 1å°æ—¶
DEFAULT_CONSUMER_WAIT_TIME = 60*1000        # æ¶ˆè´¹è€…ç­‰å¾…è¶…æ—¶æ—¶é—´(ms) 60s=1min
DEFAULT_MAX_POLL_RECORDS = 500            # æ¯æ¬¡æ¶ˆè´¹çš„pollæœ€å¤§è®°å½•æ•°


class KnowledgeSyncConsumer:
    """
    ä¸€ä¸ªä» Kafka æ¶ˆè´¹çŸ¥è¯†æ•°æ®ï¼Œç»´æŠ¤å†…å­˜ç¼“å­˜ï¼Œå¹¶å®šæœŸåˆ›å»ºå¿«ç…§çš„æ¶ˆè´¹è€…ã€‚
    """

    def __init__(self, kafka_config, redis_conn):
        self.servers = kafka_config.bootstrap_servers  # .split(",")
        self.topic = kafka_config.topic_name
        self.group_id = kafka_config.group_id
        self.client_id = kafka_config.client_id
        self.offset = kafka_config.auto_offset_reset
        self.bot_id_needed = kafka_config.bot_id_needed
        self.redis_conn = redis_conn

        self.last_snapshot_time = 0.0                               # æœ€åä¸€æ¬¡çš„å¿«ç…§æ—¶é—´
        self.consumer: Optional[KPConsumer] = None

        # --- ç±»å°è£…ï¼šqyä¸€è§¦å³è¾¾æ•°æ® å°†æ‰€æœ‰çŠ¶æ€ä¿å­˜åœ¨å®ä¾‹ä¸­ ---
        self.knowledge: Dict[str, Any] = {
            "intent2param": {},
            "param2ent": {},
            "ent2vocab": {},
            # "qa_qy_onetouch": [], 
            # "slot": {},
        }
        # ä½¿ç”¨æšä¸¾ä½œä¸º keyï¼Œæ›´å®‰å…¨
        self._source_handlers = {
            SourceType.ENTITY: self._handle_entity,
            SourceType.PARAM: self._handle_param,
            SourceType.INTENT: self._handle_intent,
        }
    
    def _load_snapshot(self) -> bool:
        """
        [å†…éƒ¨æ–¹æ³•] å¯åŠ¨æ—¶å°è¯•ä» Redis å¿«ç…§æ¢å¤ã€‚
        æˆåŠŸæ¢å¤è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
        """
        if not self.redis_conn:
            logger.warning("Redis connection not provided, cannot load snapshot.")
            return False

        try:
            logger.info("Loaded snapshot. åˆæ¬¡å¯åŠ¨ï¼Œå°è¯•ä» Redis å¿«ç…§æ¢å¤...")

            raw = self.redis_conn.get(snapshot_key)
            if not raw:
                logger.info("   No snapshot found in Redis.")
                return False

            snap = json.loads(raw)
            self.knowledge.update(snap.get("data", {}))
            offsets = snap.get("offsets", {})   # åç§»é‡

            if not offsets:
                logger.warning("    Snapshot found but contains no offset information.")
                return False

            # æ‰‹åŠ¨åˆ†é…åˆ†åŒºå’Œ topic
            tp = TopicPartition(topic=self.topic, partition=0)
            self.consumer.assign([tp])  # è¿™é‡Œæ˜¯å£°æ˜æˆ‘è¦æ‰‹åŠ¨ç®¡ç†è¿™ä¸ªconsumerçš„è¿™ä¸ªpartition
            logger.info(f"  Assigned partitions manually: {tp}")

            # ä¸ºåˆ†åŒºè®¾ç½®åç§»é‡ â†’ æœ€æ–°
            offset_to_seek = offsets.get(str(tp.partition))
            if offset_to_seek is not None:
                self.consumer.seek(tp, offset=offset_to_seek) # ä»0åŒºçš„ç¬¬offsetä½ç½®å¼€å§‹è¯»å– 
                # self.consumer.seek_to_end(tp)   # latest
                logger.info(f"  Seeking partition {tp.partition} to offset {offset_to_seek}")

            # å°†ä¸€è§¦å³è¾¾ä¸­çš„ã€æ„å›¾é—®é¢˜å’Œå®ä½“ã€‘å†™å…¥æœ¬åœ°jsonæ–‡ä»¶
            #ï¼ˆé‡å»ºç”¨äºfaqå¬å›å’Œæ§½ä½æå–ï¼‰
            _qa_qy_onetouch = self._build_onetouch_data(self.knowledge["intent2param"])
            _slot =  self._build_slot_data(self.knowledge["ent2vocab"])
            self._write_json_file(_qa_qy_onetouch, paths.kafka.onetouch)
            self._write_json_file(_slot, paths.kafka.slot)
            logger.info(f"  ã€æ„å›¾é—®é¢˜å’Œå®ä½“ã€‘å·²å†™å…¥æœ¬åœ°json â†’ {paths.kafka.onetouch} | {paths.kafka.slot}")
            # å¤‡ä»½ knowledge æ•°æ®
            self._write_json_file(self.knowledge["intent2param"], paths.kafka.intent)
            self._write_json_file(self.knowledge["param2ent"], paths.kafka.param)
            self._write_json_file(self.knowledge["ent2vocab"], paths.kafka.entity)

            logger.info(f"Loaded snapshot finished.\n")
            return True
        
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode snapshot JSON from Redis: {e}")
            return False
        except Exception as e: # Redis aof other errors
            logger.error(f"Failed to load snapshot from Redis: {e}", exc_info=True)
            return False

    def _save_snapshot(self):
        """[å†…éƒ¨æ–¹æ³•] ä¿å­˜çŸ¥è¯†åº“å’Œæ¶ˆè´¹è€…åç§»é‡åˆ° Redis å¿«ç…§ã€‚"""
        if not self.redis_conn:
            logger.warning("Redis connection not provided, cannot save snapshot.")
            return

        if not self.consumer:
            logger.warning("Consumer not running, cannot save snapshot.")
            return

        try:
            # è·å–å½“å‰åˆ†é…åˆ°çš„åˆ†åŒºçš„åç§»é‡
            # consumer.position(tp) è¿”å›ä¸‹ä¸€ä¸ªè¦æ¶ˆè´¹çš„æ¶ˆæ¯çš„åç§»é‡ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„
            offsets = {
                str(tp.partition): self.consumer.position(tp)
                for tp in self.consumer.assignment()
            }
            if not offsets:
                logger.warning("No partitions assigned, skipping snapshot with offsets.")
                return

            snap = {"data": self.knowledge, "offsets": offsets, "timestamp": time.time()}
            self.redis_conn.set(snapshot_key, json.dumps(snap))
            logger.info(f"Snapshot saved with offsets for {len(offsets)} partitions. offsets: {offsets}")
            self.last_snapshot_time = time.time() # æ›´æ–°å¿«ç…§æ—¶é—´
        except Exception as e:
            logger.error(f"Failed to save snapshot to Redis: {e}", exc_info=True)
    
    def _handle_entity(self, record: Dict, operateType: OperateType):
        """å¤„ç† ENTITY ç±»å‹çš„è®°å½•"""
        entId = record.get('entId')
        if not entId: return

        if operateType in [OperateType.INSERT, OperateType.UPDATE]:
            # å®ä½“å’Œå®ä½“å€¼æ˜¯ä¸€å¯¹å¤šå…³ç³»
            self.knowledge["ent2vocab"][entId] = {
                'entityName': record.get('entityName', ''),    # å®ä½“åç§°
                'entityType': record.get('entityType', ''),    # å®ä½“ç±»å‹ï¼ˆ  1-æšä¸¾/2-æ­£åˆ™/3-æ„å›¾/4-ç³»ç»Ÿ/5-ç®—æ³•å®ä½“ ï¼‰
                'vocab': {                                     # å®ä½“å€¼å­—å…¸ {å®ä½“å€¼:[åŒä¹‰è¯]}
                    entry['vocabularyEntryName']: [syn for syn in entry.get('synonyms', '').split('|') if syn]
                    for entry in record.get('entVocabularyEntries', [])   # å®ä½“å€¼åˆ—è¡¨
                }
            }
        elif operateType == OperateType.DELETE:
            # ä½¿ç”¨ .pop(key, None) æ›´å®‰å…¨ï¼Œå¦‚æœ key ä¸å­˜åœ¨ä¸ä¼šæŠ¥é”™
            self.knowledge["ent2vocab"].pop(entId, None)

    def _handle_param(self, record: List[Dict], operateType: OperateType):
        """å¤„ç† PARAM ç±»å‹çš„è®°å½•"""
        records = record if isinstance(record, list) else [record]

        for rec in records:
            paramId = rec.get('id')
            if not paramId: continue

            if operateType in [OperateType.INSERT, OperateType.UPDATE]:
                # åŸå§‹é€»è¾‘ä¸­ï¼ŒINSERT/UPDATE çš„ record æ˜¯ä¸€ä¸ªåˆ—è¡¨
                self.knowledge["param2ent"][paramId] = {        # å˜é‡å’Œå®ä½“æ˜¯ä¸€å¯¹ä¸€å…³ç³»
                    'paramName': rec.get('paramName', ''),      # å˜é‡ id
                    'entityId': rec.get('entityId', '')         # å…³è”å®ä½“id
                }
            elif operateType == OperateType.DELETE:
                # åŸå§‹é€»è¾‘ä¸­ï¼ŒDELETE çš„ record æ˜¯ä¸€ä¸ªå­—å…¸
                self.knowledge["param2ent"].pop(paramId, None)

    def _handle_intent(self, record: Dict, operateType: OperateType):
        """å¤„ç† INTENT ç±»å‹çš„è®°å½•"""
        intentId = record.get('id')
        if not intentId: return
        # if not record['status']=="1": return

        if operateType in [OperateType.INSERT, OperateType.UPDATE]:
            diaInteractInput = record.get('diaInteractInput', [])   # æ ‡å‡†é—®é¢˜å’Œç›¸ä¼¼é—®é¢˜åˆ—è¡¨ â†“â†“â†“
            # ä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼å’Œ next() é«˜æ•ˆæŸ¥æ‰¾ä¸»é—®é¢˜
            primary_info = next((k for k in diaInteractInput if k.get('knowledgeId') != 0 or k.get("sentenceType") == "1"), None)
            # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æŸ¥æ‰¾ç›¸ä¼¼é—®
            similar_sentences = [k['sentence'] for k in diaInteractInput if not (k.get('knowledgeId') != 0 or k.get("sentenceType") == "1")]

            # æ„å›¾å’Œå˜é‡æ˜¯ä¸€å¯¹å¤šå…³ç³»
            # intentId = record.get('intentId', '')     # æ„å›¾id
            self.knowledge["intent2param"][intentId] = {
                'intentName': record.get('intentName', ''),     # æ„å›¾åç§°
                'primaryName': record.get('name', ''),          # æ„å›¾å¯¹åº”çš„é—®æ³•
                'flowId': record.get('flowId', ''),             # æ„å›¾å…³è”çš„æµç¨‹id
                'flowName': record.get('flowName', ''),         # æ„å›¾å…³è”çš„æµç¨‹åç§°
                'paramIdList': record.get('paramIdList', []),   # æ„å›¾å…³è”çš„å˜é‡idåˆ—è¡¨
                'vocab': {  # æ„å›¾å…³è”çš„æ ‡å‡†é—®å’Œç›¸ä¼¼é—®
                    'knowledgeId': primary_info['knowledgeId'] if primary_info else None,
                    'primary': primary_info['sentence'] if primary_info else '',
                    # 'markedSentence': primary_info['markedSentence'] if primary_info else '',     # ?
                    # 'markedEntities': primary_info['markedEntities'] if primary_info else '',     # ?
                    'similary': similar_sentences
                }
            }
        elif operateType == OperateType.DELETE:
            self.knowledge["intent2param"].pop(record.get('id'), None)

    def _process_message(self, msg):
        """å¤„ç†å•æ¡ Kafka æ¶ˆæ¯"""
        try:
            # logger.info(("%s:%d:%d: key=%s value=%s" % (msg.topic, msg.partition, msg.offset, msg.key, msg.value)))
            value_dict = json.loads(msg.value)

            # # æ£€æŸ¥å­—æ®µæ˜¯å¦åˆæ³•(å¯ä»¥æ›´å…·ä½“)
            # source_str = value_dict.get('source')
            # op_type_str = value_dict.get('operateType')
            # if not all([source_str, op_type_str, 'data' in value_dict]):
            #     logger.warning(f"Message missing required fields: {msg.value}")
            #     return

            record = json.loads(value_dict['data'])  # jsonæ¶ˆæ¯
            bizTime = value_dict.get('bizTime', '') # æ—¶é—´æˆ³ timestamp
            source = value_dict.get('source', '').upper() # (æ„å›¾ã€å˜é‡ã€å®ä½“ã€BOT_DEPLOY) âœ”
            operateType = value_dict.get('operateType', '').upper()  # INSERT UPDATE DELETE BOT_DEPLOYâœ”
            # logger.info(f"{source} {operateType} | record:{record}")

            # æ ¹æ®bot_idè¿‡æ»¤ä¸ç›¸å…³çš„æ¶ˆæ¯
            if self.bot_id_needed:
                # ç»Ÿä¸€ bot_id è·å–é€»è¾‘
                if source == 'PARAM':
                    # PARAMçš„æ–°å¢æ˜¯æ‰¹é‡çš„ï¼Œæ›´æ–°å’Œåˆ é™¤æ˜¯å•ä¸ªçš„ï¼Œéœ€ç»Ÿä¸€ä¸€ä¸‹æ ¼å¼
                    if isinstance(record, dict): 
                        record = [record]
                    bot_id = record[0].get('botid', record[0].get('botId', ''))
                else:
                    bot_id = record.get('botId', record.get('botid', ''))
                
                # æ ¹æ®bot_idè¿‡æ»¤ä¸ç›¸å…³çš„æ¶ˆæ¯ï¼ˆä»…é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒï¼‰
                if bot_id and not bot_id.startswith(self.bot_id_needed):
                    return # é™é»˜å¿½ç•¥ä¸ç›¸å…³çš„æ¶ˆæ¯
            
            # # å­˜å‚¨kafkaæ¶ˆæ¯
            # write_dict_to_json(value_dict, received_kafka_data_path)

            # # æ¸…æ´—æ•°æ®
            # clean_data = self.clean_data(value_dict)
            
            if source == SourceType.BOT_DEPLOY:
                logger.info("source=BOT_DEPLOY -> self.knowledge{æ„å›¾ã€å˜é‡ã€å®ä½“} save to redis!!!")
                self._save_snapshot()
                self._update_qy_knowledge_4_faq()     # push to API
                # input("Press Enter to continue...")
            else:
                # source in [entity, param, intent]
                logger.info(f"é€æ¡æ¶ˆæ¯å¤„ç†ï¼šæ ¹æ® source={source} å’Œ operateType={operateType} åˆ†å‘ä»»åŠ¡!")
                handler = self._source_handlers.get(source)
                if handler:
                    handler(record, operateType)
                    # logger.info(f"----{msg.offset}----")
                else:
                    logger.warning(f"Unknown source type: {source}")
        
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode JSON from message value: {msg.value}. Error: {e}")
        except (KeyError, TypeError) as e:
            traceback.print_exc()
            logger.error(f"Data structure error in message: {msg.value}. Error: {e}")
        except Exception:
            traceback.print_exc()
            logger.error(f"Unexpected error processing message at offset {msg.offset}", exc_info=True)

    def _processing_loop(self):
        """ä¸»æ¶ˆè´¹å¾ªç¯"""
        logger.info("Consumer started. Waiting for messages loop...")
        while True:
            # --- ä¼˜åŒ–: æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„å®šæ—¶å¿«ç…§é€»è¾‘ ---
            # æ¯éš” 1h=3600s åšä¸€æ¬¡å¿«ç…§ï¼ˆå¯æ¢æˆæŒ‰æ¡æ•°æˆ–å¼‚æ­¥çº¿ç¨‹ï¼‰
            if time.time() - self.last_snapshot_time > SNAPSHOT_INTERVAL_SECONDS:
                self._save_snapshot()

            # ä½¿ç”¨ poll è·å–æ‰¹é‡æ¶ˆæ¯ï¼Œæä¾›è¶…æ—¶æ—¶é—´å‚æ•°timeout_msæ¥æ§åˆ¶æ›´æ–°é¢‘ç‡
            # å¦‚æœåœ¨60s=1minå†…å¦‚æœæ²¡æœ‰æ¶ˆæ¯å¯ç”¨ï¼Œè¿”å›ä¸€ä¸ªç©ºé›†åˆ
            msg_pack = self.consumer.poll(timeout_ms=DEFAULT_CONSUMER_WAIT_TIME)
            if not msg_pack:
                continue

            # éå†æ¯ä¸ªæ‰¹æ¬¡çš„æ¶ˆæ¯å­—å…¸
            for tp, messages in msg_pack.items():
                if len(messages) > 1:
                    logger.info(f"Processing-loop {len(messages)} messages from partition {tp.partition}\n")
                for msg in messages:
                    self._process_message(msg)

    # --- ä¼˜åŒ–: åˆå§‹åŒ– Kafka consumerï¼Œå¹¶å¤„ç†å¿«ç…§åŠ è½½ ---
    def _init_snap_consumer(self):
        """åˆå§‹åŒ– Kafka æ¶ˆè´¹è€…ï¼Œå¹¶å¤„ç†å¿«ç…§åŠ è½½"""
        consumer_config = {
            "bootstrap_servers": self.servers,
            "group_id": self.group_id,
            "client_id": self.client_id,
            "auto_offset_reset": self.offset,  # 'earliest' æˆ– 'latest'
            "enable_auto_commit": False,  # æ‰‹åŠ¨ç®¡ç†åç§»é‡ï¼Œæ‰€ä»¥å…³é—­è‡ªåŠ¨æäº¤
            "value_deserializer": lambda v: v.decode("utf-8", "ignore"),
            "key_deserializer": lambda k: k.decode("utf-8", "ignore"),
            "max_poll_records": DEFAULT_MAX_POLL_RECORDS,
            # æ ¹æ®éœ€è¦æ·»åŠ  SASL é…ç½®
        }
        self.consumer = KPConsumer(**consumer_config)
        logger.info(f"Kafka consumer initialized successfully for topic: {self.topic}")
        
        # å°è¯•ä»å¿«ç…§åŠ è½½ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™å›é€€åˆ°æ ‡å‡†çš„è®¢é˜…æ¨¡å¼
        if not self._load_snapshot():
            self.consumer.subscribe([self.topic])  # è®¢é˜…æŒ‡å®šçš„ä¸»é¢˜
            logger.info("No snapshot loaded, subscribing to topic to get partitions automatically.")
        
        # è®°å½•å¿«ç…§æ—¶é—´
        self.last_snapshot_time = time.time()

        # # è·å–ä¸»é¢˜çš„åˆ†åŒºä¿¡æ¯
        # logger.info(self.consumer.partitions_for_topic(self.topic))  # {0}
        # # # è·å–ä¸»é¢˜åˆ—è¡¨
        # # logger.info(self.consumer.topics())
        # # è·å–å½“å‰æ¶ˆè´¹è€…è®¢é˜…çš„ä¸»é¢˜
        # logger.info(self.consumer.subscription())   # {'sync_lixiang_intent_data'}
        # # è·å–å½“å‰æ¶ˆè´¹è€…topicã€åˆ†åŒºä¿¡æ¯
        # logger.info(self.consumer.assignment())     # {TopicPartition(topic='sync_lixiang_intent_data', partition=0)}
        # # è·å–å½“å‰ä¸»é¢˜{0}çš„æœ€æ–°åç§»é‡
        # tp = TopicPartition(topic=self.topic, partition=0)
        # offset = self.consumer.position(tp)  # <class 'int'>
        # logger.info(f"åˆ†åŒº0 for topic '{self.topic}' çš„æœ€æ–°åç§»é‡: {offset}")
        # # psï¼šã€æœªæŒ‡å®šåˆ†åŒºã€‘+ã€æ¶ˆæ¯æœªæŒ‡å®škeyã€‘-> éšæœºåœ°å‘é€åˆ° topic å†…çš„æ‰€æœ‰å¯ç”¨åˆ†åŒº{0} -> å¯ä¿è¯æ¶ˆè´¹æ¶ˆæ¯çš„é¡ºåºæ€§

    def run(self):
        """å¯åŠ¨æ¶ˆè´¹è€…ï¼ŒåŒ…å«åˆå§‹åŒ–å’Œä¸»æ¶ˆè´¹å¾ªç¯"""
        try:
            self._init_snap_consumer()

            # æ•è· SIGTERM ä¿¡å·å¹¶ä¼˜é›…é€€å‡º
            def graceful_shutdown(sig, frame):
                logger.info("Received SIGTERM, saving snapshot and closing consumer...")
                self._save_snapshot()  # ä¿å­˜å¿«ç…§
                self.consumer.close()  # å…³é—­æ¶ˆè´¹è€…
                sys.exit(0)

            signal.signal(signal.SIGTERM, graceful_shutdown)
            signal.signal(signal.SIGINT, graceful_shutdown) # ä¹Ÿå¤„ç† Ctrl+C

            self._processing_loop()
        
        except KafkaError as e:
            logger.error(f"A Kafka error occurred: {e}")
        except Exception:
            traceback.print_exc()
            logger.critical("An unexpected error caused the consumer to stop.", exc_info=True)
        finally:
            if self.consumer:
                logger.info("Closing Kafka consumer in finally block.")
                self.consumer.close()


    @staticmethod
    def clean_data(data):
        """æ¸…æ´—æ•°æ®ï¼Œæ•°æ®è½¬æ¢ (ä¿æŒä¸å˜)"""
        clean_data = data.copy()
        clean_data["event_id"] = clean_data.pop("df_event_id")
        # ... å…¶ä»–æ¸…æ´—é€»è¾‘
        return clean_data

    # --- ä¼˜åŒ–: æ‹†åˆ† `update_qy_knowledge`ï¼Œä½¿å…¶èŒè´£æ›´æ¸…æ™° ---
    @staticmethod
    def _build_onetouch_data(intent2param) -> List[Dict]:
        """1ã€æŠŠåŒæ­¥çš„æ„å›¾æ•°æ®-è¦†ç›–æ›´æ–°qa_qy_onetouch.csv"""
        rows = []
        ff = lambda x: x.strip('\n').strip()
        # intent2param = self.knowledge["intent2param"]
        for _intent_id, content in intent2param.items():
            pri_question = content['primaryName']
            sim_questions = content["vocab"]['similary']
            qid = str(content["vocab"]["knowledgeId"])
            rows.append(
                {
                    "answer_content_list": [], 
                    "id": qid,
                    "question_id": qid,
                    "question_content": ff(pri_question),
                    "question_type": 0,    #    -- é—®é¢˜ç±»å‹ï¼ˆ0æ ‡å‡†é—®é¢˜ã€1çŸ¥è¯†ç‚¹ï¼‰
                    "similar_question_list": [
                        {
                            "question_id": qid,
                            "similar_id": qid + f"_{i+1}",
                            "similar_question": q
                        }
                        for i, q in enumerate(sim_questions)
                    ],
                    "category_all_name": "",
                    "status": 1,    # å·²å¯ç”¨
                    # "longEffective": 1,   # é•¿æœŸæœ‰æ•ˆ
                    "valid_begin_time": "",
                    "valid_end_time": "", 
                    "source": "ä¸€è§¦å³è¾¾æ„å›¾",    # ä¸ƒé±¼ä¸€è§¦å³è¾¾
                }
            )
        return rows

    @staticmethod
    def _build_slot_data(ent2vocab) -> Dict[str, List[str]]:
        """2ã€å°†æšä¸¾å®ä½“å†™å…¥ slot.json (entityType=1)"""
        slot2vocab = {}
        for eid, val in ent2vocab.items():
            if val.get("entityType") == 1:  # æšä¸¾å®ä½“
                for entity, ev_list in val.get('vocab', {}).items():
                    entity_key = f"{val.get('entityName', '')}-{entity}"
                    slot2vocab[entity_key] = ev_list + [entity]
        return slot2vocab

    @staticmethod   
    def _write_json_file(data: Any, path: Path):
        """é€šç”¨ JSON æ–‡ä»¶å†™å…¥å‡½æ•°"""
        try:
            with codecs.open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
            logger.info(f"  Successfully wrote data to {path}")
        except IOError as e:
            logger.error(f"  Failed to write to file {path}: {e}")

    def _trigger_downstream_update(self):
        """3ã€åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­å¼‚æ­¥è§¦å‘ä¸‹æ¸¸æœåŠ¡æ›´æ–°"""
        def _update_task():
            print("faqå‘é‡æ›´æ–°......")
            print(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time())))
            try:
                update_api_url: str = "http://127.0.0.1:8098/update_faq_onetouch",
                headers = {"Content-Type": "application/json"}
                response = requests.get(update_api_url, headers=headers, timeout=30)
                response.raise_for_status() # å¦‚æœçŠ¶æ€ç æ˜¯ 4xx æˆ– 5xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                logger.info(f"Downstream update triggered successfully. Status: {response.status_code}")
            except requests.exceptions.RequestException as e:
                logger.error(f"Failed to trigger downstream update: {e}")
            print(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time())))
            print("ã€faqå‘é‡æ›´æ–°ã€‘å®Œæˆ!\n")
        
        # è¯·æ±‚æœ¬åœ°updateæ¥å£ï¼Œæ›´æ–°å…¨å±€å˜é‡ï¼ˆfaiss & BM25 & slotï¼‰
        thread = threading.Thread(target=_update_task)
        thread.start()
        # thread.join()   # ç­‰å¾…çº¿ç¨‹å®Œæˆ
        print("=======faissæ›´æ–°å®Œæˆï¼Œæ¶ˆæ¯å­—å…¸æ¸…ç©º=======")

    def _update_qy_knowledge_4_faq(self):
        """
        æ‰§è¡ŒçŸ¥è¯†éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼š
        1. ç”Ÿæˆå¹¶å†™å…¥ä¸€è§¦å³è¾¾æ–‡ä»¶ã€‚
        2. ç”Ÿæˆå¹¶å†™å…¥ slot æ–‡ä»¶ã€‚
        3. å¼‚æ­¥è§¦å‘ä¸‹æ¸¸æœåŠ¡æ›´æ–°ã€‚
        """
        # å°†ä¸€è§¦å³è¾¾ä¸­çš„ã€æ„å›¾é—®é¢˜å’Œå®ä½“ã€‘å†™å…¥æœ¬åœ°jsonæ–‡ä»¶
        #ï¼ˆé‡å»ºç”¨äºfaqå¬å›å’Œæ§½ä½æå–ï¼‰
        _qa_qy_onetouch = self._build_onetouch_data(self.knowledge["intent2param"])
        _slot =  self._build_slot_data(self.knowledge["ent2vocab"])
        self._write_json_file(_qa_qy_onetouch, paths.kafka.onetouch)
        self._write_json_file(_slot, paths.kafka.slot)
        logger.info(f"ã€æ„å›¾é—®é¢˜å’Œå®ä½“ã€‘å·²å†™å…¥æœ¬åœ°json â†’ {paths.kafka.onetouch} | {paths.kafka.slot}")
        # å¤‡ä»½ knowledge æ•°æ®
        self._write_json_file(self.knowledge["intent2param"], paths.kafka.intent)
        self._write_json_file(self.knowledge["param2ent"], paths.kafka.param)
        self._write_json_file(self.knowledge["ent2vocab"], paths.kafka.entity)
        
        # å¼‚æ­¥è§¦å‘ä¸‹æ¸¸æ›´æ–°
        logger.info("å¼€å§‹è§¦å‘ä¸‹æ¸¸æ›´æ–°....")
        self._trigger_downstream_update()


def manual_sync_knowledge_snapshot():
    """æ‰‹åŠ¨å°†æœ¬åœ°ä¸ƒé±¼çŸ¥è¯†å¿«ç…§ push to Redis
    ï¼ˆç¬¬ä¸€æ¬¡éƒ¨ç½²/è°ƒè¯•ç”¨ï¼‰
    """
    # åˆ é™¤å¿«ç…§ï¼ˆæµ‹è¯•ï¼‰
    redis_client = RedisUtilsSentinel(redis_config.__dict__)
    redis_client.delete(snapshot_key)
    logger.info(f"åˆ é™¤å¿«ç…§æˆåŠŸï¼š{snapshot_key}")
    # è¯»å–å·²æ¶ˆè´¹/ç¼“å­˜æ•°æ®
    with open(paths.kafka.intent, 'r', encoding='utf-8') as file:
        json_string = file.read()
        intent2param = json.loads(json_string)
    with open(paths.kafka.param, 'r', encoding='utf-8') as file:
        json_string = file.read()
        param2ent = json.loads(json_string)
    with open(paths.kafka.entity, 'r', encoding='utf-8') as file:
        json_string = file.read()
        one_entity_dict = json.loads(json_string)
    with open(paths.entities.sys, 'r', encoding='utf-8') as file:
        json_string = file.read()
        sys_entity_dict = json.loads(json_string)
        sys_entity_dict = {x['id']:{"entityName": x["entityName"],"entityType": 0,"vocab": {}} for x in sys_entity_dict}
    with open(paths.entities.algo, 'r', encoding='utf-8') as file:
        json_string = file.read()
        algo_entity_dict = json.loads(json_string)
        algo_entity_dict = {x['id']:{"entityName": x["entityName"],"entityType": 5,"vocab": {}} for x in algo_entity_dict}
    ent2vocab = {**one_entity_dict, **sys_entity_dict, **algo_entity_dict}  # åŠ å…¥ç³»ç»Ÿå®ä½“å’Œç®—æ³•å®ä½“ â†’ entity.json
    
    knowledges = {
        "intent2param": intent2param,
        "param2ent": param2ent,
        "ent2vocab": ent2vocab,
    }

    # è·å–å½“å‰ä¸»é¢˜{0}çš„æœ€æ–°åç§»é‡
    _consumer = KPConsumer(
        bootstrap_servers=kafka_config.bootstrap_servers,
        group_id=None,
        client_id=None,
        enable_auto_commit=False,
    )
    tp = TopicPartition(topic=kafka_config.topic_name, partition=0)
    # è·å–æœ€æ—©å’Œæœ€æ–°åç§»é‡
    earliest_offsets = _consumer.beginning_offsets([tp])[tp]
    latest_offsets = _consumer.end_offsets([tp])[tp]
    # print(f"Topic '{kafka_config.topic_name}' çš„ earliest_offsets: {earliest_offsets}")  # 2567
    # print(f"Topic '{kafka_config.topic_name}' çš„ latest_offsets: {latest_offsets}")      # 148770
    _consumer.close()

    # æ‰‹åŠ¨åŒæ­¥å¿«ç…§
    snap = {"data": knowledges, "offsets": {'0':latest_offsets}, "timestamp": time.time()}
    redis_client.set(snapshot_key, json.dumps(snap))
    logger.info("æ‰‹åŠ¨è¯»å–æœ¬åœ°çš„ä¸ƒé±¼çŸ¥è¯†å¿«ç…§ï¼Œå¹¶ä¿å­˜åˆ° Redis ğŸ‘Œ")

    # ç”Ÿæˆqa_qy_onetouch.json å’Œ slot.json
    _qa_qy_onetouch = KnowledgeSyncConsumer._build_onetouch_data(knowledges["intent2param"])
    _slot =  KnowledgeSyncConsumer._build_slot_data(knowledges["ent2vocab"])
    KnowledgeSyncConsumer._write_json_file(_qa_qy_onetouch, paths.kafka.onetouch)
    KnowledgeSyncConsumer._write_json_file(_slot, paths.kafka.slot)


def main(kafka_config, redis_config):
    try:
        # Redis å¤šæœºå…±äº«å¿«ç…§
        redis_client = RedisUtilsSentinel(redis_config.__dict__)
        logger.info("Successfully connected to Redis.")
    except Exception as e:
        logger.error(f"Could not connect to Redis: {e}. Snapshots will not be available.")
        redis_client = None
    
    # åˆ›å»ºå¹¶è¿è¡Œæ¶ˆè´¹è€…
    kfk_consumer = KnowledgeSyncConsumer(
        kafka_config=kafka_config,
        redis_conn=redis_client,
    )
    kfk_consumer.run()


if __name__ == "__main__":

    # # åˆ é™¤å¿«ç…§ï¼ˆæµ‹è¯•ï¼‰
    # redis_client = RedisUtilsSentinel(redis_config.__dict__)
    # redis_client.delete(snapshot_key)
    # logger.info(f"åˆ é™¤å¿«ç…§æˆåŠŸï¼š{snapshot_key}")
    # exit()

    # # ä¸ƒé±¼çŸ¥è¯†å¿«ç…§ â†’ push to Redisï¼ˆç¬¬ä¸€æ¬¡éƒ¨ç½²/è°ƒè¯•ç”¨ï¼‰
    # manual_sync_knowledge_snapshot()
    # exit()

    # è¿è¡Œæ¶ˆè´¹è€…
    main(kafka_config, redis_config)
