import os
import sys
# os.environ["CUDA_VISIBLE_DEVICES"] = "3"
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.getcwd())
sys.path.append(os.path.abspath('.'))

from fastapi import FastAPI, HTTPException
from fastapi import status
from fastapi.responses import JSONResponse, Response
from fastapi import APIRouter, Depends, File, UploadFile, Query

import re
from datetime import datetime
import importlib
import traceback
from pydantic import BaseModel, validator
from starlette.requests import Request
from starlette.testclient import TestClient
import uvicorn
import asyncio
from typing import List, Dict, Optional, Any
from collections import OrderedDict
import json
import requests
import torch
import gc
import pandas as pd

from uniqa.api.request.faq_schema import *
from uniqa.api.response import response_code
from uniqa.logging import Loggers, logDog, log_filter

from uniqa.api.faq import FAQPipeline, DataPreprocessor
from uniqa import Document
from configs.config import *

"""
gunicorn uniqa.api.api:app --bind=0.0.0.0:8091 --workers=1 -k uvicorn.workers.UvicornWorker
nohup gunicorn uniqa.api.api:router -c configs/gunicorn_config_api.py > logs/api.log 2>&1 &
"""

app = FastAPI(title="FAQ API")

# åˆå§‹åŒ–ç»„ä»¶
preprocessor = DataPreprocessor()
docs = preprocessor.load_data(data_path="uniqa/data/demo.json")

# Do Metadata Filtering
filters={
    "operator": "AND",
    "conditions": [
        # {"field": "score", "operator": ">", "value": 0.3},    # è¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦ç»“æœ(émetaæ•°æ®âŒ)
        # {"field": "meta.answes[*].status", "operator": "==", "value": 1},   # Milvus ä¸æ”¯æŒåµŒå¥—å­—æ®µæŸ¥è¯¢âŒ
        {"field": "meta.status", "operator": "==", "value": 1},  # âœ“
        # è¿‡æ»¤è¿‡æœŸé—®é¢˜ âœ“
        {
            "operator": "OR",
            "conditions": [
                {"field": "meta.valid_begin_time", "operator": "==", "value": None},
                {"field": "meta.valid_begin_time", "operator": "<=", "value": str(datetime.now())},
            ],
        }, 
        {
            "operator": "OR",
            "conditions": [
                {"field": "meta.valid_end_time", "operator": "==", "value": None},
                {"field": "meta.valid_end_time", "operator": ">=", "value": str(datetime.now())},
            ],
        },
        # TEXT_MATCH
    ]
}

# åˆå§‹åŒ–QA bot
faq = FAQPipeline(is_whitening=False)
faq.setup_milvusDB_retriever(docs, filters)


def torch_gc():
    """é‡Šæ”¾PyTorchçš„GPUç¼“å­˜"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„CUDAè®¾å¤‡
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡
        print(f"gpuå†…å­˜æ¸…ç†å®Œæˆï¼")
        # gc.collect()
        # print(f"åƒåœ¾å›æ”¶å®Œæˆï¼")
    except Exception as e:
        print(f"æ¸…ç†å†…å­˜æ—¶å‡ºé”™: {e}")


# # åŠ è½½ Milvusï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œï¼‰
# @app.on_event("startup")
# async def startup_event():
#     faq.setup_milvusDB_retriever(docs, filters)


@app.get("/")
async def read_root():
    return {"message": "Welcome to the FAQ API"}


@app.post("/search")
@log_filter
def search_faq(request: Request, item: Item):
    """æ ¹æ®è¾“å…¥çš„æŸ¥è¯¢å­—ç¬¦ä¸²æœç´¢ç›¸å…³çš„ FAQ æ¡ç›®
    """
    item = item.dict()
    query = item["query"]   # å¦‚ä½•æ›´æ–°OTAç³»ç»Ÿ ï½œ æ€æ ·å‡çº§è½¦è½½ç³»ç»Ÿ ï½œ è½¯ä»¶æ›´æ–°æ–¹æ³•
    top_k = item["top_k"]  # æ£€ç´¢å¬å›æ•°é‡ï¼Œé»˜è®¤5
    search_strategy = item["search_strategy"]  # æ£€ç´¢ç­–ç•¥ï¼ˆå¯é€‰ sparse/embedding/hybridï¼‰
    car_type = item["car_type"]         # list[str]=[]
    ota_version = item["ota_version"]   # list[str]=[]  

    # è¾“å…¥éªŒè¯
    if not isinstance(top_k, int) or top_k <= 0:
        return response_code.resp_4001(data="topKå¿…é¡»æ˜¯æ­£æ•´æ•°ã€‚")
    if search_strategy not in ['hybrid', 'sparse', 'embedding']:
        return response_code.resp_4001(data="æ£€ç´¢ç­–ç•¥é”™è¯¯æˆ–æš‚ä¸æ”¯æŒ...")
    if not query:
        return response_code.resp_4001(data="è¾“å…¥ä¸èƒ½ä¸ºç©º")
    
    # # å¼‚å¸¸æƒ…å†µå¤„ç†
    # if isinstance(query, str):
    #     query = [query]
    # elif not isinstance(query, list):
    #     return response_code.resp_4001(data="è¾“å…¥æ•°æ®ç±»å‹é”™è¯¯")

    # query = list(map(_text_standardize, query))

    # ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢å¬å›ç›¸å…³æ–‡æ¡£
    results = faq.run(query, top_k=top_k, search_strategy=search_strategy)
    # results = results[:top_k]

    # ç¬¬äºŒæ­¥ï¼šå¤æ‚filtersé€»è¾‘ â†’ ç­›é€‰ç­”æ¡ˆ
    serialized_results = []
    user_conditions = {"car_type": car_type, "ota_version": ota_version}
    for doc in results:
        # æå–è¯¥é—®é¢˜çš„æ‰€æœ‰ç­”æ¡ˆ
        answers = doc.meta.get("answers", [])

        # è®¡ç®—ç­”æ¡ˆä¸ç”¨æˆ·æ¡ä»¶çš„åŒ¹é…åº¦
        score_list = []
        for ans in answers:
            if ans.get("status") != 1:  # è¿‡æ»¤ç­”æ¡ˆçº§æ— æ•ˆçŠ¶æ€
                continue
            match_score = faq._match_conditions(ans, user_conditions)  # æ¡ä»¶åŒ¹é…åº¦ï¼ˆ0~1ï¼‰
            score_list.append(match_score)
            # # ç»¼åˆå¾—åˆ† = é—®é¢˜è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆdoc.scoreï¼‰ + æ¡ä»¶åŒ¹é…åº¦ï¼ˆmatch_scoreï¼‰
            # combined_score = round(doc.score + match_score, 4)

        # ç­›é€‰ score_listä¸­å¤§äºç­‰äº0.7 çš„index
        index_list = [i for i, score in enumerate(score_list) if score >= 0.7]
        if index_list:
            serialized_results.append(
                {
                    "question": doc.content,  # åŒ¹é…çš„é—®é¢˜
                    "question_id": doc.meta.get("question_id"),
                    "question_type": doc.meta.get("question_type"),
                    "category": doc.meta.get("category", ""), 
                    "is_main_question": doc.meta.get("is_main_question"),
                    "score": round(float(doc.score), 4),    # fastapi ä¸æ”¯æŒnp.floatç±»å‹
                    "source": 1,
                    "answer": [answers[i] for i in index_list]  # ç¬¦åˆæ¡ä»¶çš„ç­”æ¡ˆ
                }
            )
        else:
            # è·å–é»˜è®¤ç­”æ¡ˆç´¢å¼•ï¼ˆè½¦å‹æ ‡ç­¾ã€ç”Ÿæ•ˆæ—¶é—´ã€æœ€é«˜/æœ€ä½otaç‰ˆæœ¬éƒ½ä¸ºç©ºçš„å…ƒç´ ï¼‰
            default_index = None
            for i, ans in enumerate(answers):
                if not ans.get("car_type") and not ans.get("effective_time") and not ans.get("max_ota_version") and not ans.get("min_ota_version"):
                    default_index = i
                    break
            # æ·»åŠ é»˜è®¤ç­”æ¡ˆ
            if default_index is not None and default_index not in index_list:
                serialized_results.append(
                {
                    "question": doc.content,  # åŒ¹é…çš„é—®é¢˜
                    "question_id": doc.meta.get("question_id"),
                    "question_type": doc.meta.get("question_type"),
                    "category": doc.meta.get("category", ""), 
                    "is_main_question": doc.meta.get("is_main_question"),
                    "score": round(float(doc.score), 4),    # fastapi ä¸æ”¯æŒnp.floatç±»å‹
                    "source": 1,
                    "answer": answers[default_index]  # é»˜è®¤ç­”æ¡ˆ
                }
            )

    # å°†ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ï¼Œå­˜å‚¨åœ¨serialized_results
    # æŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼Œå–Top Kç­”æ¡ˆ
    serialized_results.sort(key=lambda x: x["score"], reverse=True)
    serialized_results = serialized_results[:top_k]     # å³detail_results
    # logDog.info(serialized_results)
    return response_code.resp_200(data={"query": query, "results": serialized_results})


@app.get("/full_update")
def full_update(request: Request):
    """æ›´æ–°å…¨å±€å˜é‡"""
    try:
        logDog.info("å¼€å§‹å…¨é‡æ›´æ–°FAQç³»ç»Ÿ...")

        global faq
        preprocessor = DataPreprocessor()
        docs = preprocessor.load_data(data_path="uniqa/data/demo.json")
        faq = FAQPipeline(is_whitening=False)
        faq.setup_milvusDB_retriever(docs, filters)

        torch_gc()  # éšç€æ›´æ–°æ¬¡æ•°å¢å¤šï¼Œæ˜¾å­˜å ç”¨ä¼šå˜å¤§ï¼Œæ‰€ä»¥é¡¶ä¸€ä¸ª torch_gc() æ–¹æ³•å®Œæˆå¯¹æ˜¾å­˜çš„å›æ”¶

        logDog.info("FAQç³»ç»Ÿå…¨é‡æ›´æ–°æˆåŠŸ\n")
        return {'status': True, 'msg': 'Successfully updated all FAQ components'}
    except Exception as e:
        error_msg = f"ã€full_update æ¥å£ã€‘æ›´æ–°FAQç³»ç»Ÿæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logDog.error(f"{error_msg}\n{traceback.format_exc()}")
        return {'status': False, 'msg': error_msg}


@app.get("/incremental_update")
def incremental_update(request: Request):
    """å¢é‡æ›´æ–°FAQçŸ¥è¯†åº“ 
    ç»Ÿä¸€é€šè¿‡ write_documents å’Œ delete_documents å®ç°å¢åˆ æ”¹ï¼Œè€Œæ— éœ€æ¯æ¬¡éƒ½é‡æ–°ç´¢å¼•æ‰€æœ‰æ•°æ®ã€‚
    """
    try:
        # è·å– Milvus ä¸­æ‰€æœ‰æ–‡æ¡£çš„ id å’Œ meta
        # æ³¨æ„ï¼šå¯¹äºéå¸¸å¤§çš„æ•°æ®é›†ï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ–‡æ¡£å¯èƒ½æ•ˆç‡ä¸é«˜ã€‚
        # æ›´å¥½çš„æ–¹æ³•æ˜¯åˆ†æ‰¹è·å–æˆ–ä½¿ç”¨æ›´é«˜çº§çš„åŒæ­¥ç­–ç•¥ï¼ˆè§åæ–‡ï¼‰ã€‚
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è·å–å…¨éƒ¨ã€‚
        milvus_docs = faq.milvus_document_store.filter_documents()
        milvus_data_map = {doc.id: doc.meta for doc in milvus_docs}

        # --- 3. æ›´æ–°å’Œæ·»åŠ  (UPDATE & ADD) ---
        print("\n--- 3. æ›´æ–°å’Œæ·»åŠ  (UPDATE & ADD) ---")

        # æ¨¡æ‹Ÿæ•°æ®æ›´æ–°ï¼šç­”æ¡ˆå†…å®¹å˜äº†ï¼Œå¹¶ä¸”å¢åŠ äº†ä¸€ä¸ªæ–°çš„ç›¸ä¼¼é—®é¢˜
        update_and_add_docs = preprocessor.load_data("uniqa/data/demo2.json")
        print(f"ä»æ›´æ–°åçš„ JSON ç”Ÿæˆäº† {len(update_and_add_docs)} ä¸ª Haystack Document:")
        for doc in update_and_add_docs:
            print(f"  - ID: {doc.id}, Content: '{doc.content}'")

        # åµŒå…¥å¹¶ä½¿ç”¨ OVERWRITE ç­–ç•¥å†™å…¥
        from uniqa.document_stores.types import DuplicatePolicy
        embedded_update_docs = faq.doc_embedder.run(documents=update_and_add_docs)["documents"]
        # update_documents
        count = faq.milvus_document_store.write_documents(
            embedded_update_docs,
            policy=DuplicatePolicy.OVERWRITE
        )

        # print(f"\nå¢é‡æ›´æ–°æ“ä½œå†™å…¥äº† {count} ä»½æ–‡æ¡£ã€‚")
        # print(f"å½“å‰ DocumentStore ä¸­çš„æ–‡æ¡£æ€»æ•°: {faq.milvus_document_store.count_documents()}")

        # # éªŒè¯æ›´æ–°ç»“æœ
        # print("\nğŸ” éªŒè¯æ›´æ–°ç»“æœ:")
        # # æ£€æŸ¥ä¸»é—®é¢˜æ–‡æ¡£æ˜¯å¦è¢«æ›´æ–°
        # updated_doc_kb001 = faq.milvus_document_store.filter_documents(
        #     filters={"operator": "AND", "conditions": [{"field": "id", "operator": "==", "value": "KBXX1"}] }
        # )[0]
        # print(f"ID 'KBXX1' çš„ç­”æ¡ˆå·²æ›´æ–°: '{updated_doc_kb001.meta['answers'][0]['answerContent']}'")
        # # æ£€æŸ¥æ–°æ·»åŠ çš„ç›¸ä¼¼é—®é¢˜æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        # new_doc_kb001_3 = faq.milvus_document_store.filter_documents(
        #     filters={"operator": "AND", "conditions": [{"field": "id", "operator": "==", "value": "KBXX1_4"}] }
        # )
        # if new_doc_kb001_3:
        #     print("ID 'KBXX1_4' çš„æ–°ç›¸ä¼¼é—®é¢˜å·²æˆåŠŸæ·»åŠ ã€‚")
        # else:
        #     print("é”™è¯¯ï¼šæ–°ç›¸ä¼¼é—®é¢˜æ·»åŠ å¤±è´¥ï¼")

        torch_gc()  # éšç€æ›´æ–°æ¬¡æ•°å¢å¤šï¼Œæ˜¾å­˜å ç”¨ä¼šå˜å¤§ï¼Œæ‰€ä»¥é¡¶ä¸€ä¸ª torch_gc() æ–¹æ³•å®Œæˆå¯¹æ˜¾å­˜çš„å›æ”¶

        logDog.info(f"\nå¢é‡æ›´æ–°æ“ä½œå†™å…¥äº† {count} ä»½æ–‡æ¡£ã€‚")
        logDog.info(f"å½“å‰ DocumentStore ä¸­çš„æ–‡æ¡£æ€»æ•°: {faq.milvus_document_store.count_documents()}")
        return {'status': True, 'msg': f'Successfully updated {count} entries'}
    
    except Exception as e:
        error_msg = f"ã€incremental_update æ¥å£ã€‘ å¢é‡æ›´æ–°ç´¢å¼•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logDog.error(f"{error_msg}\n{traceback.format_exc()}")
        return {'status': False, 'msg': error_msg}


# =========================== #


@app.get("/update_faq_know/{item_id}")
def update_faq_know(item_id: int):
    """
    å¢é‡æ›´æ–°FAQçŸ¥è¯†åº“
    Args:
        item_id (int): æ›´æ–°ç±»å‹ - 1: æ ‡å‡†é—®å¢é‡æ›´æ–°, 2: ç›¸ä¼¼é—®å¢é‡æ›´æ–°
    """
    try:
        file_path = None
        if item_id == 1:
            # çŸ¥è¯†æ ‡å‡†é—®å¢é‡åŒæ­¥å­˜å‚¨
            file_path = './data_factory/update_robot_know.csv'
        elif item_id == 2:
            # ç›¸ä¼¼é—®å¢é‡åŒæ­¥å­˜å‚¨
            file_path = './data_factory/update_robot_know_sim.csv'
        else:
            error_msg = 'Invalid item_id, must be 1 or 2'
            logDog.error(error_msg)
            return {'status': False, 'msg': error_msg}
        
        logDog.info(f"å¼€å§‹ know={item_id} çŸ¥è¯†å¢é‡æ›´æ–°")
        
        # è¯»å–CSVæ–‡ä»¶
        # ç©ºå€¼éƒ½æ˜¯é»˜è®¤ä¸º NANï¼Œè®¾ç½® keep_default_na=False è®©è¯»å–å‡ºæ¥çš„ç©ºå€¼æ˜¯ç©ºå­—ç¬¦ä¸²
        df = pd.read_csv(file_path, encoding="utf-8", keep_default_na=False)

        if df.empty:
            logDog.info(f"æ–‡ä»¶ {file_path} ä¸ºç©ºï¼Œæ— éœ€æ›´æ–°")
            return {'status': True, 'msg': 'No data to update'}

        # æ”¶é›†æ–°å¥å­
        new_sentences = []
        new_qid_dict = {}
        new_sen2qid = OrderedDict()

        # å¤„ç†æ ‡å‡†é—®æ›´æ–°
        if item_id==1:
            for i,row in df.iterrows():
                question_id = row["question_id"]
                question_content = row["question_content"]
                answer_content = row["answer_content"]
                car_type = row["car_type"]

                # è·å–ç°æœ‰ç›¸ä¼¼é—®ï¼ˆå¦‚æœæœ‰ï¼‰
                similar_sentence = faq_sys.recall_module.faiss.qid_dict.get(
                    question_id, {}).get("similar_sentence", [])

                # æ›´æ–°é—®é¢˜-ç­”æ¡ˆå­—å…¸
                new_qid_dict[question_id] = {
                    'standard_sentence': question_content,
                    'similar_sentence': similar_sentence,
                    'answer': answer_content,
                    'source': "çŸ¥è¯†åº“",
                    'car_type': car_type
                }
                # æ›´æ–°é—®é¢˜-IDæ˜ å°„å’Œå¥å­åˆ—è¡¨
                new_sen2qid[question_content] = question_id
                new_sentences.append(question_content)

        # å¤„ç†ç›¸ä¼¼é—®æ›´æ–°
        if item_id==2:
            for i, row in df.iterrows():
                question_id = row["question_id"]
                similar_question = row["similar_question"]
                # æ£€æŸ¥question_idæ˜¯å¦åœ¨ç°æœ‰è¯å…¸ä¸­
                if question_id not in faq_sys.recall_module.faiss.qid_dict:
                    logDog.warning(f"æ ‡å‡†é—®ID {question_id} ä¸å­˜åœ¨ï¼Œç›¸ä¼¼é—® '{similar_question}' å°†ä½¿ç”¨å ä½æ ‡å‡†é—®")
                        
                # è·å–ç°æœ‰æ•°æ®
                question_content = faq_sys.recall_module.faiss.qid_dict.get(
                    question_id, {}).get("standard_sentence", "æ ‡å‡†é—®æ’å…¥å»¶è¿Ÿ")
                similar_sentence = faq_sys.recall_module.faiss.qid_dict.get(
                    question_id, {}).get("similar_sentence", []).copy()  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                answer_content = faq_sys.recall_module.faiss.qid_dict.get(
                    question_id, {}).get("answer_content", "")
                car_type = faq_sys.recall_module.faiss.qid_dict.get(
                    question_id, {}).get("car_type", "")
                
                # æ·»åŠ æ–°çš„ç›¸ä¼¼é—®
                similar_sentence.append(similar_question)

                # æ›´æ–°é—®é¢˜-ç­”æ¡ˆå­—å…¸
                new_qid_dict[question_id] = {
                    'standard_sentence': question_content,
                    'similar_sentence': similar_sentence,
                    'answer': answer_content,
                    'source': "çŸ¥è¯†åº“",
                    'car_type': car_type
                }
                # æ›´æ–°é—®é¢˜-IDæ˜ å°„å’Œå¥å­åˆ—è¡¨
                new_sen2qid[similar_question] = question_id
                new_sentences.append(similar_question)

        # å¦‚æœæœ‰æ–°å¥å­éœ€è¦æ·»åŠ åˆ°å½“å‰ç´¢å¼•
        if new_sentences:
            logDog.info(f"æ·»åŠ  {len(new_sentences)} æ¡æ–°å¥å­åˆ°ç´¢å¼•")
            # æ›´æ–°è¯å…¸å’Œæ˜ å°„
            faq_sys.recall_module.faiss.qid_dict.update(new_qid_dict)
            faq_sys.recall_module.faiss.sen2qid.update(new_sen2qid)
            faq_sys.recall_module.faiss.sentences.extend(new_sentences)
            # ç”Ÿæˆå¹¶æ·»åŠ æ–°å‘é‡
            new_vecs = faq_sys.recall_module.faiss.get_vecs(new_sentences)
            new_vecs = faq_sys.recall_module.faiss.__tofloat32__(new_vecs)
            faq_sys.recall_module.faiss.index.add(new_vecs)

            # æ›´æ–°å…¨å±€å˜é‡
            global global_sentences, global_qid_dict, global_sen2qid
            global_sentences.extend(new_sentences)
            global_qid_dict.update(new_qid_dict)
            global_sen2qid.update(new_sen2qid)

        torch_gc()  # éšç€æ›´æ–°æ¬¡æ•°å¢å¤šï¼Œæ˜¾å­˜å ç”¨ä¼šå˜å¤§ï¼Œæ‰€ä»¥é¡¶ä¸€ä¸ª torch_gc() æ–¹æ³•å®Œæˆå¯¹æ˜¾å­˜çš„å›æ”¶

        logDog.info(f"know={item_id} çŸ¥è¯†å¢é‡æ›´æ–°æˆåŠŸ\n")
        return {'status': True, 'msg': f'Successfully updated {len(new_sentences)} entries'}
    except Exception as e:
        error_msg = f"ã€update_faq_knowæ¥å£ã€‘know={item_id} å¢é‡æ›´æ–°ç´¢å¼•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logDog.error(f"{error_msg}\n{traceback.format_exc()}")
        return {'status': False, 'msg': error_msg}


if __name__ == "__main__":
    config = uvicorn.Config(
        "api:app", 
        host='0.0.0.0', port=8091, 
        workers=1, 
        # limit_concurrency=200,  # æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ None
        # reload=True,
        # debug=True,
    )
    server = uvicorn.Server(config)
    # å°†uvicornè¾“å‡ºçš„å…¨éƒ¨è®©loguruç®¡ç†
    Loggers.init_config()
    server.run()
    