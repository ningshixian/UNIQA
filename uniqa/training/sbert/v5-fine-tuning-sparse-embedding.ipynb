{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/train-sparse-encoder\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/sparse_encoder/applications/retrieve_rerank/hybrid_search.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么使用稀疏嵌入模型？\n",
    "简而言之，神经稀疏嵌入模型在 BM25 等传统词汇方法和 Sentence Transformers 等密集嵌入模型之间占据着以下优势：\n",
    "\n",
    "混合潜力：与密集模型非常有效地结合，在词汇匹配很重要的搜索中可能会遇到困难\n",
    "可解释性：你可以准确地看到哪些标记有助于匹配\n",
    "性能：在许多检索任务中具有竞争力或优于密集模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SparseEncoder\n",
    "\n",
    "# Download from the 🤗 Hub\n",
    "model = SparseEncoder(\"naver/splade-v3\")\n",
    "\n",
    "# Run inference\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# (3, 30522)\n",
    "\n",
    "# Get the similarity scores for the embeddings\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[   32.4323,     5.8528,     0.0258],\n",
    "#         [    5.8528,    26.6649,     0.0302],\n",
    "#         [    0.0258,     0.0302,    24.0839]])\n",
    "\n",
    "# Let's decode our embeddings to be able to interpret them\n",
    "decoded = model.decode(embeddings, top_k=10)\n",
    "for decoded, sentence in zip(decoded, sentences):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此示例中，嵌入是 30,522 维向量，其中每个维度对应于模型词汇表中的一个标记。该decode方法返回了嵌入中值最高的 10 个标记，这使我们能够解读哪些标记对嵌入的贡献最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SparseEncoder\n",
    "\n",
    "model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")\n",
    "\n",
    "model = SparseEncoder(\"google-bert/bert-base-uncased\")\n",
    "# SparseEncoder(\n",
    "#   (0): MLMTransformer({'max_seq_length': 512, 'do_lower_case': False, 'architecture': 'BertForMaskedLM'})\n",
    "#   (1): SpladePooling({'pooling_strategy': 'max', 'activation_function': 'relu', 'word_embedding_dimension': None})\n",
    "# )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
